描述一个事件

* 线性代数：状态与变化
* 概率：  确信度
* 实际： 无法被穷尽，各式各样的变体
      
`高考：`
>方向一： 
    训练更多的数据：题海战术；
    
>方向二：
    加入先验证知识：调整假设空间；

` 维度诅咒：`

### 关于f的寻找
 * 能够拟合训练集的关联f并非唯一
* 光有大数据是不够的，难点在与测试集的表现。
* 无法保证在训练集上表现良好的数据在测试集上也同样表现良好


###无免费午餐(No Free for Lunch Theorem)
`Any two optimization algorithms are equivalent when their performance is averaged across all 
possible problems.`

###分布式表达 :（Distributed respresentation:  to distentangle the factors of variation.）

###神经网络
`x w y`

`y = relu(Wx+b)`  ： 神经网络最基本的变化公式；
x:输入
w:控制拿多少不同的原子
relu() ：非线性
b:偏移，超过该阀值才能激活
y:输出

###`迭代变换：`
分布式表达是将变体拆分成因素。
但是如果用无限结点的浅层网络，所拆分的变体并不会再不同的样本之间形成共享。

>在浅层网络中止负责学习自己的关联，而在深层网络中。那些共用相同因素的样本也会被间接的训练到。
深层神经网络加入了第二条先验知识：迭代变换；

`关键：因素的共享；`


###深层神经网络
* 学习的过程是因素间的关系的拆分；
* 关系间的拆分是信息的回卷，
* 信息的回卷是变体的消除，
* 变体的消除是不确定性的缩减。


`自然界两个固有的先验知识：`
>并行：新状态是由若干个因素并行组合形成；
>
>迭代：新转态由已形成的状态再次迭代形成。

### 应用：设计理念（核心部分）

### 基本概念：
* 拆分因素：将变体拆分因素，降低所需数据量；
* 因素共享：使所拆分的因素在在不同的样本之间形成共享，可以用等量的数据训练出更好的模型；

* 误区1： 深层学习，并非万能，应用先验知识的前提，是数据可以 以先验知识的方式生成。
* 误区2：深层学习没有固定形式，加入不同先验知识，形成不同神经网络。

`每一层表示事物的一种状态，设计神经网络时，要以 “层”为单元。`

### 神经网络变体

* 循环层：***时间共享***,
如果用前馈层，每个圆圈表示100个结点，那么前馈层处理时序相关性时，就需要学习300个权重；
但如果世道不同权重在时间下共享的，那么就需要学习200个权重。

* 卷积层 ： ***空间共享***，如果用前馈层，那么学习81个权重，但如果知道这些权重在***空间下是共享***的，那么可能只需要学习9个权重，原本一张图片在前馈层中只能用于一次权重，在卷积层却可以学习很多次。



### 设计自己的神经网络

>神经网络并不黑箱，真正黑箱的是你的task

>设计神经网络就是寻找你手头的task上，利用因素分解和因素共享的合理方式

>可以先经过前馈层，再经过双向循环层，再经过前馈层，最终得到你的结果。



`sample`

* 迁移学习(Transfer learning)：
 `反光----f1---->视网膜成像---f2--->脑中概念`
 
 ` x--------------h----------------y`
  利用因素共享这一特点，将一个任务中学习到的关联，应用到其他任务当中去。
  
  
* 多任务学习(Multi task learning)
 > 各自知识，共享知识，二次元输入，三次元输入
 > 先验知识：与迁移学习一样利用***因素共享*** 这一特点
 > 作   用：扩增额外训练数据，约束想要的关联f
 
* 跨层组合
 先验知识：跨层组合，
 前馈神经网络是不允许跨层组合的，但现实中是否有跨层组合的现象？
 `代表网络：残差网络`
> 眼睛+身形+着装 --> 来判断一个人
* 蒸馏模型：其本质还是迁移学习，ta迁移的是标签，先训练老师模型，以其训练值，作为一种额外的标签，

### 自动编码器
>* inputer layer
>* "bottle neck" hidden layer (all layers are fully connected but not drawn)
>* outputer layer(reconstruction of inputer layer)
> * 自动编码器是对利用并行与迭代的这两个先验知识，来操控变体的一种技术，
> * to disentangle the factors of variaton，
> * 可用作生成模型，也可用做特征工程，
 
  

### 调整假设空间--`4个原则`
> * 增加共享（降低数据量）
> * 增加惩罚（对那些不符合理念的范围进行惩罚）
> * 优化起点 （优先从哪里找起）
> * 降低变体数目（数据预处理）

### 网络结构的设计
* 因素拆分，因素共享
### 网络学习的优化
* 梯度下降所遇到的问题，或寻找其他优化方式。

### Batch Normalization
* 输入层
* 隐藏层
* 输出层
* 在***隐藏层上来标准化***，从而降低变体数量，降低拟合难度；
*

### 端到端（End to end）
* 输入：细胞层
* 输出：系统层

### 抑制过拟合（Overfitting）
***```dropout(遗忘)：```***细节有时候也能形成规律，但不会每次都形成。
遗忘可以去掉那些偶然型号才能的细节规律，提高普遍性。

***```shuffle(乱序)：```*** 训练的样本不要有固定顺序，而要随机打乱，同样可以抑制偶然形成的细节规律。比如不要这一会从abandon 开始背单词一样；

***```L2 regularization(保持简单化)：```***了解的方案不要过于复杂，不然只能顾及特例而失去普遍性。

***```mini-batch(多题一起做)：```*** 相互比较后得出结论，比如同时看两本不同描述的书，可以得到更好的理解。

***```noisy layer(加噪音)：```*** 题目加入一些干扰项，改变考前环境，教室，平时状态等。

 











